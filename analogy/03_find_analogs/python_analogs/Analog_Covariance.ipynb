{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# basic libraries\n",
    "#\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.covariance import EmpiricalCovariance, MinCovDet\n",
    "import sklearn.covariance\n",
    "from scipy.spatial import distance\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "# plotting stuff\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import collections\n",
    "import seaborn as sb\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 8, 6\n",
    "from timeit import default_timer as timer\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ALL_location_NNs(future_df, analog_pool, numeric_feat, NN_count):\n",
    "    local_sites = future_df.location.unique()\n",
    "    #\n",
    "    # initiate dataframes to speed up\n",
    "    # \n",
    "    row_count = future_df.shape[0]\n",
    "    needed_col_count = NN_count * 2\n",
    "\n",
    "    all_NNs_df   = future_df[['year', 'location']].copy() # data frame containing (year, location)\n",
    "    all_dists_df = future_df[['year', 'location']].copy() # data frame containing distances\n",
    "\n",
    "\n",
    "    ## concatenate new data frame to above ones, to speed up\n",
    "    NNs_new_cols, dists_new_cols = create_colnames(NN_count)\n",
    "\n",
    "    NNs_df_help = pd.DataFrame('-999', index = all_NNs_df.index,  columns = NNs_new_cols)\n",
    "    dists_df_help = pd.DataFrame('-999', index = all_dists_df.index,  columns = dists_new_cols)\n",
    "\n",
    "    all_NNs_df = pd.concat([all_NNs_df, NNs_df_help], axis=1)\n",
    "    all_dists_df = pd.concat([all_dists_df, dists_df_help], axis=1)\n",
    "\n",
    "    del(NNs_df_help, dists_df_help, NNs_new_cols, dists_new_cols)\n",
    "\n",
    "    for loc in local_sites:\n",
    "        # pick up one location data\n",
    "        curr_loc_df = future_df[future_df.location == loc].copy()\n",
    "        complete_hist_df = analog_pool.copy()\n",
    "        \n",
    "        output = find_1_location_NNs_builtin(curr_location_df = curr_loc_df, \n",
    "                                             complete_hist_df = analog_pool, \n",
    "                                             numeric_feat=numeric_feat, \n",
    "                                             NN_count=NN_count)\n",
    "        \n",
    "        all_NNs_df[all_NNs_df.index.isin(list(output[0].index))] = output[0]\n",
    "        all_dists_df[all_NNs_df.index.isin(list(output[1].index))] = output[1]\n",
    "        del(output)\n",
    "    return(all_NNs_df, all_dists_df)\n",
    "\n",
    "def find_1_location_NNs_builtin(curr_location_df, complete_hist_df, numeric_feat, NN_count, meter=\"euclidean\"):\n",
    "    curr_location_df_cp = curr_location_df.copy()\n",
    "    complete_hist_df_cp = complete_hist_df.copy()\n",
    "    \n",
    "    future_yr_count = curr_location_df_cp.shape[0]\n",
    "    needed_col_count = NN_count * 2\n",
    "    \n",
    "    NNs_df   = curr_location_df_cp[['year', 'location']].copy() # data frame containing (year, location)\n",
    "    dists_df = curr_location_df_cp[['year', 'location']].copy() # data frame containing distances\n",
    "    \n",
    "    ## concatenate new data frame to above ones, to speed up\n",
    "    NNs_df_new_cols, dists_df_new_cols = create_colnames(NN_count)\n",
    "    \n",
    "    NNs_df_helper = pd.DataFrame('-999', index=NNs_df.index,  columns=NNs_df_new_cols)\n",
    "    dists_df_helper = pd.DataFrame('-999', index=dists_df.index,  columns=dists_df_new_cols)\n",
    "    \n",
    "    NNs_df = pd.concat([NNs_df, NNs_df_helper], axis=1)\n",
    "    dists_df = pd.concat([dists_df, dists_df_helper], axis=1)\n",
    "    \n",
    "    del(NNs_df_helper, dists_df_helper, NNs_df_new_cols, dists_df_new_cols)\n",
    "    \n",
    "    \n",
    "    # form the ICV to compute its covariance to remove inter-annual variability\n",
    "    ICV = complete_hist_df.copy()\n",
    "    ICV = ICV.loc[ICV['location'] == curr_location_df_cp.location.unique()[0]] # filter corresponding location\n",
    "    #############################################################################\n",
    "    #\n",
    "    #          Normalize before doing anything\n",
    "    #\n",
    "    #############################################################################\n",
    "    ICV_means = ICV.loc[:, numeric_feat].mean()\n",
    "    ICV_stds = ICV.loc[:, numeric_feat].std()\n",
    "    ICV_stds[ICV_stds.le(10**(-10))] = 1\n",
    "    \n",
    "    ICV = (ICV.loc[:, numeric_feat] - ICV_means) / ICV_stds\n",
    "    curr_location_df_cp.loc[:, numeric_feat] = (curr_location_df_cp.loc[:, numeric_feat] - ICV_means) / ICV_stds\n",
    "    complete_hist_df.loc[:, numeric_feat] = (complete_hist_df.loc[:, numeric_feat] - ICV_means) / ICV_stds\n",
    "    #\n",
    "    # pick numerical part of the data frame to do the operations:\n",
    "    #\n",
    "    complete_hist_df_numeric = complete_hist_df.loc[:, numeric_feat].copy()\n",
    "    future_numeric = curr_location_df_cp.loc[:, numeric_feat].copy()\n",
    "    ICV = ICV.loc[:, numeric_feat]\n",
    "    \n",
    "    ### Apply PCA here and use those to find analogs\n",
    "    pca = PCA(n_components = detect_effective_compon(ICV))\n",
    "    pca.fit(ICV);\n",
    "    #\n",
    "    # transform data into PCA space to compute analogs\n",
    "    ICV_pca = pca.transform(ICV)\n",
    "    hist_pca = pca.transform(complete_hist_df_numeric)\n",
    "    future_pca = pca.transform(future_numeric)\n",
    "\n",
    "    # the following is the same as [(1/N) * np.matmul(M.transpose(), M)]. which is not even divided by N-1\n",
    "    # cov = sklearn.covariance.empirical_covariance(ICV_pca, assume_centered=False)\n",
    "    \n",
    "    # there is no difference between the following line and adding metric_params={'V': cov} to it\n",
    "    neigh = NearestNeighbors(n_neighbors=NN_count, metric = meter, algorithm=\"brute\")\n",
    "    neigh.fit(hist_pca);\n",
    "    for yr in np.arange(2):\n",
    "#         print(\"future_pca[yr, ]\")\n",
    "#         print(future_pca[yr, ])\n",
    "#         print(\"________________________\")\n",
    "        result = neigh.kneighbors([future_pca[yr, ]])\n",
    "\n",
    "        NNs_distances = result[0][0]\n",
    "        NNs_idx = result[1][0]\n",
    "#         print (\"NNs_distances:\")\n",
    "#         print (NNs_distances)\n",
    "#         print (\"_________________\")\n",
    "#         print (\"NNs_idx\")\n",
    "#         print (NNs_idx)\n",
    "#         print (\"_________________\")\n",
    "        # find and reshape the NNs\n",
    "        # reshape the nearest neighbros from long to wide, so, every other column is (year, location) of ith NN\n",
    "        curr_NNs = complete_hist_df.loc[NNs_idx, ['year', 'location']].copy()\n",
    "        curr_NNs = list(np.hstack(np.split(curr_NNs, NN_count))[0])\n",
    "        NNs_df.iloc[yr, 2:] = curr_NNs\n",
    "        dists_df.iloc[yr, 2:] = NNs_distances\n",
    "    return(NNs_df, dists_df)\n",
    "\n",
    "def filter_locations(all_dt, local_dt):\n",
    "    # list of unique locations in the data\n",
    "    local_sites = local_dt.location.unique()\n",
    "    all_sites = all_dt.location.unique()\n",
    "\n",
    "    # find the local sites that exist in all_usa_data\n",
    "    local_sites = np.intersect1d(local_sites, all_sites)\n",
    "\n",
    "    # select the rows corresponding to existing sites\n",
    "    local_dt = local_dt.loc[local_dt['location'].isin(local_sites)]\n",
    "    return (local_dt)\n",
    "\n",
    "def detect_effective_compon(matriks):\n",
    "    n_comp = matriks.shape[1]\n",
    "    pca = PCA(n_components = n_comp)\n",
    "    pca.fit(matriks)\n",
    "    return (len(pca.explained_variance_[pca.explained_variance_ > 0.01])) \n",
    "\n",
    "def create_colnames(NN_count):\n",
    "    year_loc_cols = pd.Series(['year_NN_', 'location_NN_'] * NN_count)\n",
    "    numbers = pd.Series(np.arange(1, NN_count+1).repeat(2))\n",
    "    year_loc_cols = year_loc_cols.astype(str) + numbers.astype(str)\n",
    "    year_loc_cols = list(year_loc_cols)\n",
    "    \n",
    "    dist_cols = pd.Series(['dist_NN_'] * NN_count)\n",
    "    dist_cols = list(pd.Series(['dist_NN_'] * NN_count) + pd.Series(np.arange(1, NN_count+1)).astype(str))\n",
    "    return (year_loc_cols, dist_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ALL_location_NNs_not_efficient(future_df, analog_pool, numeric_feat, NN_count):\n",
    "    local_sites = future_df.location.unique()\n",
    "    #\n",
    "    # initiate dataframes to speed up\n",
    "    #\n",
    "    needed_col_count = NN_count * 2\n",
    "\n",
    "    NNs_new_cols, dists_new_cols = create_colnames(NN_count)\n",
    "    NNs_new_cols = ['year', 'location'] + NNs_new_cols\n",
    "    dists_new_cols = ['year', 'location'] + dists_new_cols\n",
    "    # data frame containing (year, location)\n",
    "    all_NNs_df = pd.DataFrame(columns=dists_new_cols)\n",
    "    \n",
    "    # data frame containing distances\n",
    "    all_dists_df = pd.DataFrame(columns=np.arange(NN_count+2))\n",
    "    \n",
    "    all_NNs_df = pd.DataFrame()\n",
    "    all_dists_df = pd.DataFrame()\n",
    "\n",
    "    for loc in local_sites:\n",
    "        # pick up one location data\n",
    "        curr_loc_df = future_df[future_df.location == loc].copy()\n",
    "        complete_hist_df = analog_pool.copy()\n",
    "        \n",
    "        output = find_1_location_NNs_builtin(curr_location_df = curr_loc_df, \n",
    "                                             complete_hist_df = analog_pool, \n",
    "                                             numeric_feat=numeric_feat, \n",
    "                                             NN_count=NN_count)\n",
    "        \n",
    "        all_NNs_df = pd.concat([all_NNs_df, output[0]])\n",
    "        all_dists_df = pd.concat([all_dists_df, output[1]])\n",
    "        del(output)\n",
    "    return(all_NNs_df, all_dists_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = \"/Users/hn/Desktop/Desktop/Kirti/check_point/analogs/\"\n",
    "out_dir = \"/Users/hn/Desktop/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine needed columns\n",
    "If we want to drop some columns like `Gen_4`, `preci`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_3_drop = False\n",
    "gen_4_drop = False\n",
    "precip_drop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['medianDoY', 'NumLarvaGens_Aug', 'mean_escaped_Gen1', 'mean_escaped_Gen2', 'mean_escaped_Gen3', 'mean_escaped_Gen4', 'mean_precip', 'mean_gdd']\n"
     ]
    }
   ],
   "source": [
    "numeric_feat = ['medianDoY', 'NumLarvaGens_Aug', \n",
    "                'mean_escaped_Gen1', 'mean_escaped_Gen2', 'mean_escaped_Gen3', 'mean_escaped_Gen4', \n",
    "                'mean_precip', 'mean_gdd']\n",
    "\n",
    "non_numeric_feat = ['year', 'location', 'ClimateScenario']\n",
    "\n",
    "if gen_3_drop == True:\n",
    "    numeric_feat.remove('mean_escaped_Gen3')\n",
    "\n",
    "if gen_4_drop == True:\n",
    "    numeric_feat.remove('mean_escaped_Gen4')\n",
    "\n",
    "if precip_drop == True:\n",
    "    numeric_feat.remove('mean_precip')\n",
    "\n",
    "print(numeric_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_orig = pd.read_csv(in_dir + \"all_data_usa.csv\")\n",
    "hist_orig = hist_orig.loc[:, non_numeric_feat + numeric_feat] # drop unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_orig = pd.read_csv(in_dir + \"averaged_data_rcp45.csv\")\n",
    "future_orig = future_orig.loc[:, non_numeric_feat + numeric_feat] # drop unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# future_orig = future_orig.iloc[0:1000].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver\n",
    "\n",
    "\n",
    "#### Filter the locations\n",
    "Some locations in local data are not in all USA. So, here we choose the local (future) data in whose\n",
    "sites do exist in all_usa_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_orig = filter_locations(hist_orig, future_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick up one location data\n",
    "# curr_location_df = future_orig[future_orig.location == future_orig.location.iloc[0]].copy()\n",
    "# complete_hist_df = hist_orig.copy()\n",
    "# curr_location_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_dtt = future_orig[future_orig.location=='43.59375_-116.78125'].copy()\n",
    "all_NNs_df, all_dists_df = find_ALL_location_NNs_not_efficient(future_df = future_dtt, \n",
    "                                                               analog_pool=hist_orig, \n",
    "                                                               numeric_feat=numeric_feat, \n",
    "                                                               NN_count=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_def = all_NNs_df[all_NNs_df.location=='43.59375_-116.78125']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_NNs_df.iloc[:, 2:].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_NNs_df_0, all_dists_df_0 = find_ALL_location_NNs_not_efficient(future_df = future_dtt.iloc[0:1, :], \n",
    "                                                               analog_pool=hist_orig, \n",
    "                                                               numeric_feat=numeric_feat, \n",
    "                                                               NN_count=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_NNs_df_1, all_dists_df_1 = find_ALL_location_NNs_not_efficient(future_df = future_dtt.iloc[1:2, :], \n",
    "                                                                   analog_pool=hist_orig, \n",
    "                                                                   numeric_feat=numeric_feat, \n",
    "                                                                   NN_count=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.concat([all_NNs_df_1, all_NNs_df_0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.iloc[:, 2:].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_df = future_orig.copy()\n",
    "analog_pool = hist_orig.copy()\n",
    "complete_hist_df = analog_pool.copy()\n",
    "local_sites = future_df.location.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = local_sites[0]\n",
    "curr_loc_df_0 = future_df[future_df.location == loc].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = \"43.59375_-116.84375\"\n",
    "curr_loc_df = future_df[future_df.location == loc].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_loc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_loc_df_0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICV_0 = complete_hist_df.copy()\n",
    "ICV_0 = ICV_0.loc[ICV_0['location'] == curr_loc_df_0.location.unique()[0]] # filter corresponding location\n",
    "ICV_means_0 = ICV_0.loc[:, numeric_feat].mean()\n",
    "ICV_stds_0 = ICV_0.loc[:, numeric_feat].std()\n",
    "ICV_stds_0[ICV_stds_0.le(10**(-10))] = 1\n",
    "ICV_0 = (ICV_0.loc[:, numeric_feat] - ICV_means_0) / ICV_stds_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICV = complete_hist_df.copy()\n",
    "ICV = ICV.loc[ICV['location'] == curr_loc_df.location.unique()[0]] # filter corresponding location\n",
    "ICV_means = ICV.loc[:, numeric_feat].mean()\n",
    "ICV_stds = ICV.loc[:, numeric_feat].std()\n",
    "ICV_stds[ICV_stds.le(10**(-10))] = 1\n",
    "ICV = (ICV.loc[:, numeric_feat] - ICV_means) / ICV_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICV.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICV_0.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_loc_df_0.loc[:, numeric_feat] = (curr_loc_df_0.loc[:, numeric_feat] - ICV_means_0) / ICV_stds_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_loc_df.loc[:, numeric_feat] = (curr_loc_df.loc[:, numeric_feat] - ICV_means) / ICV_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_loc_df_0.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_loc_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_hist_df_0 = complete_hist_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_hist_df_0.loc[:, numeric_feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_hist_df_0.loc[:, numeric_feat]=(complete_hist_df_0.loc[:, numeric_feat] - ICV_means_0) / ICV_stds_0\n",
    "complete_hist_df.loc[:, numeric_feat] = (complete_hist_df.loc[:, numeric_feat] - ICV_means) / ICV_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_hist_df_0.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_hist_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_hist_df_numeric_0 = complete_hist_df_0.loc[:, numeric_feat].copy()\n",
    "complete_hist_df_numeric_0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_hist_df_numeric = complete_hist_df.loc[:, numeric_feat].copy()\n",
    "complete_hist_df_numeric.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_numeric_0 = curr_loc_df_0.loc[:, numeric_feat].copy()\n",
    "future_numeric_0.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_numeric = curr_loc_df.loc[:, numeric_feat].copy()\n",
    "future_numeric.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICV_0 = ICV_0.loc[:, numeric_feat]\n",
    "ICV_0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICV = ICV.loc[:, numeric_feat]\n",
    "ICV.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_0 = PCA(n_components = detect_effective_compon(ICV_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = detect_effective_compon(ICV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(ICV_0);\n",
    "pca.fit(ICV);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICV_pca_0 = pca.transform(ICV_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICV_pca = pca.transform(ICV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_pca_0 = pca.transform(complete_hist_df_numeric_0)\n",
    "hist_pca_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_pca = pca.transform(complete_hist_df_numeric)\n",
    "hist_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_pca_0 = pca.transform(future_numeric_0)\n",
    "future_pca_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_pca = pca.transform(future_numeric)\n",
    "future_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_count=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh_0 = NearestNeighbors(n_neighbors=NN_count, metric = \"mahalanobis\", algorithm=\"brute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = NearestNeighbors(n_neighbors=NN_count, metric = \"mahalanobis\", algorithm=\"brute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh_0.fit(hist_pca_0)\n",
    "neigh.fit(hist_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_yr_count_0 = curr_loc_df_0.shape[0]\n",
    "needed_col_count = NN_count * 2\n",
    "\n",
    "NNs_df_0   = curr_loc_df_0[['year', 'location']].copy() # data frame containing (year, location)\n",
    "dists_df_0 = curr_loc_df_0[['year', 'location']].copy() # data frame containing distances\n",
    "\n",
    "## concatenate new data frame to above ones, to speed up\n",
    "NNs_df_new_cols_0, dists_df_new_cols_0 = create_colnames(NN_count)\n",
    "\n",
    "NNs_df_helper_0 = pd.DataFrame('-999', index = NNs_df_0.index,  columns = NNs_df_new_cols_0)\n",
    "dists_df_helper_0 = pd.DataFrame('-999', index = dists_df_0.index,  columns = dists_df_new_cols_0)\n",
    "\n",
    "NNs_df_0 = pd.concat([NNs_df_0, NNs_df_helper_0], axis=1)\n",
    "dists_df_0 = pd.concat([dists_df_0, dists_df_helper_0], axis=1)\n",
    "\n",
    "del(NNs_df_helper_0, dists_df_helper_0, NNs_df_new_cols_0, dists_df_new_cols_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_yr_count = curr_loc_df.shape[0]\n",
    "needed_col_count = NN_count * 2\n",
    "\n",
    "NNs_df   = curr_loc_df[['year', 'location']].copy() # data frame containing (year, location)\n",
    "dists_df = curr_loc_df[['year', 'location']].copy() # data frame containing distances\n",
    "\n",
    "## concatenate new data frame to above ones, to speed up\n",
    "NNs_df_new_cols, dists_df_new_cols = create_colnames(NN_count)\n",
    "\n",
    "NNs_df_helper = pd.DataFrame('-999', index=NNs_df.index,  columns=NNs_df_new_cols)\n",
    "dists_df_helper = pd.DataFrame('-999', index=dists_df.index,  columns=dists_df_new_cols)\n",
    "\n",
    "NNs_df = pd.concat([NNs_df, NNs_df_helper], axis=1)\n",
    "dists_df = pd.concat([dists_df, dists_df_helper], axis=1)\n",
    "\n",
    "del(NNs_df_helper, dists_df_helper, NNs_df_new_cols, dists_df_new_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for yr in np.arange(curr_loc_df_0.shape[0]):\n",
    "    result_0 = neigh_0.kneighbors([future_pca_0[yr, ]])\n",
    "    NNs_distances_0 = result_0[0][0]\n",
    "    NNs_idx_0 = result_0[1][0]\n",
    "\n",
    "    curr_NNs_0 = complete_hist_df_0.loc[NNs_idx_0, ['year', 'location']].copy()\n",
    "    curr_NNs_0 = list(np.hstack(np.split(curr_NNs_0, NN_count))[0])\n",
    "    NNs_df_0.iloc[yr, 2:] = curr_NNs_0\n",
    "      \n",
    "    dists_df_0.iloc[yr, 2:] = NNs_distances_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for yr in np.arange(curr_loc_df.shape[0]):\n",
    "    result = neigh.kneighbors([future_pca_0[yr, ]])\n",
    "    NNs_distances = result[0][0]\n",
    "    NNs_idx = result[1][0]\n",
    "\n",
    "    curr_NNs = complete_hist_df.loc[NNs_idx, ['year', 'location']].copy()\n",
    "    curr_NNs = list(np.hstack(np.split(curr_NNs, NN_count))[0])\n",
    "    NNs_df.iloc[yr, 2:] = curr_NNs\n",
    "      \n",
    "    dists_df.iloc[yr, 2:] = NNs_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNs_df_0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists_df_0.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for yr in np.arange(curr_location_df.shape[0]):\n",
    "    result = neigh.kneighbors([future_pca[yr, ]])\n",
    "    NNs_distances = result[0][0]\n",
    "    NNs_idx = result[1][0]\n",
    "    \n",
    "    curr_NNs = complete_hist_df.loc[NNs_idx, ['year', 'location']].copy()\n",
    "    curr_NNs = list(np.hstack(np.split(curr_NNs, NN_count))[0])\n",
    "    NNs_df.iloc[yr, 2:] = curr_NNs\n",
    "        \n",
    "    dists_df.iloc[yr, 2:] = NNs_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_1_location_NNs_builtin(curr_location_df, complete_hist_df, numeric_feat, NN_count, meter=\"euclidean\"):\n",
    "    curr_location_df_cp = curr_location_df.copy()\n",
    "    complete_hist_df_cp = complete_hist_df.copy()\n",
    "    \n",
    "    future_yr_count = curr_location_df_cp.shape[0]\n",
    "    needed_col_count = NN_count * 2\n",
    "    \n",
    "    NNs_df   = curr_location_df_cp[['year', 'location']].copy() # data frame containing (year, location)\n",
    "    dists_df = curr_location_df_cp[['year', 'location']].copy() # data frame containing distances\n",
    "    \n",
    "    ## concatenate new data frame to above ones, to speed up\n",
    "    NNs_df_new_cols, dists_df_new_cols = create_colnames(NN_count)\n",
    "    \n",
    "    NNs_df_helper = pd.DataFrame('-999', index=NNs_df.index,  columns=NNs_df_new_cols)\n",
    "    dists_df_helper = pd.DataFrame('-999', index=dists_df.index,  columns=dists_df_new_cols)\n",
    "    \n",
    "    NNs_df = pd.concat([NNs_df, NNs_df_helper], axis=1)\n",
    "    dists_df = pd.concat([dists_df, dists_df_helper], axis=1)\n",
    "    \n",
    "    del(NNs_df_helper, dists_df_helper, NNs_df_new_cols, dists_df_new_cols)\n",
    "    \n",
    "    \n",
    "    # form the ICV to compute its covariance to remove inter-annual variability\n",
    "    ICV = complete_hist_df.copy()\n",
    "    ICV = ICV.loc[ICV['location'] == curr_location_df_cp.location.unique()[0]] # filter corresponding location\n",
    "    #############################################################################\n",
    "    #\n",
    "    #          Normalize before doing anything\n",
    "    #\n",
    "    #############################################################################\n",
    "    ICV_means = ICV.loc[:, numeric_feat].mean()\n",
    "    ICV_stds = ICV.loc[:, numeric_feat].std()\n",
    "    ICV_stds[ICV_stds.le(10**(-10))] = 1\n",
    "    \n",
    "    ICV = (ICV.loc[:, numeric_feat] - ICV_means) / ICV_stds\n",
    "    curr_location_df_cp.loc[:, numeric_feat] = (curr_location_df_cp.loc[:, numeric_feat] - ICV_means) / ICV_stds\n",
    "    complete_hist_df.loc[:, numeric_feat] = (complete_hist_df.loc[:, numeric_feat] - ICV_means) / ICV_stds\n",
    "    #\n",
    "    # pick numerical part of the data frame to do the operations:\n",
    "    #\n",
    "    complete_hist_df_numeric = complete_hist_df.loc[:, numeric_feat].copy()\n",
    "    future_numeric = curr_location_df_cp.loc[:, numeric_feat].copy()\n",
    "    ICV = ICV.loc[:, numeric_feat]\n",
    "    \n",
    "    ### Apply PCA here and use those to find analogs\n",
    "    pca = PCA(n_components = detect_effective_compon(ICV))\n",
    "    pca.fit(ICV);\n",
    "    #\n",
    "    # transform data into PCA space to compute analogs\n",
    "    ICV_pca = pca.transform(ICV)\n",
    "    hist_pca = pca.transform(complete_hist_df_numeric)\n",
    "    future_pca = pca.transform(future_numeric)\n",
    "\n",
    "    # the following is the same as [(1/N) * np.matmul(M.transpose(), M)]. which is not even divided by N-1\n",
    "    # cov = sklearn.covariance.empirical_covariance(ICV_pca, assume_centered=False)\n",
    "    \n",
    "    # there is no difference between the following line and adding metric_params={'V': cov} to it\n",
    "    neigh = NearestNeighbors(n_neighbors=NN_count, metric = meter, algorithm=\"brute\")\n",
    "    neigh.fit(hist_pca);\n",
    "    for yr in np.arange(2):\n",
    "#         print(\"future_pca[yr, ]\")\n",
    "#         print(future_pca[yr, ])\n",
    "#         print(\"________________________\")\n",
    "        result = neigh.kneighbors([future_pca[yr, ]])\n",
    "\n",
    "        NNs_distances = result[0][0]\n",
    "        NNs_idx = result[1][0]\n",
    "#         print (\"NNs_distances:\")\n",
    "#         print (NNs_distances)\n",
    "#         print (\"_________________\")\n",
    "#         print (\"NNs_idx\")\n",
    "#         print (NNs_idx)\n",
    "#         print (\"_________________\")\n",
    "        # find and reshape the NNs\n",
    "        # reshape the nearest neighbros from long to wide, so, every other column is (year, location) of ith NN\n",
    "        curr_NNs = complete_hist_df.loc[NNs_idx, ['year', 'location']].copy()\n",
    "        curr_NNs = list(np.hstack(np.split(curr_NNs, NN_count))[0])\n",
    "        NNs_df.iloc[yr, 2:] = curr_NNs\n",
    "        dists_df.iloc[yr, 2:] = NNs_distances\n",
    "    return(NNs_df, dists_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_of_int = future_orig[future_orig.location == \"43.59375_-116.78125\"]\n",
    "NN_count = 5\n",
    "meter = \"euclidean\" # mahalanobis, \n",
    "\n",
    "curr_location_df = site_of_int.copy()\n",
    "complete_hist_df = hist_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_location_df_cp = curr_location_df.copy()\n",
    "complete_hist_df_cp = complete_hist_df.copy()\n",
    "\n",
    "future_yr_count = curr_location_df_cp.shape[0]\n",
    "needed_col_count = NN_count * 2\n",
    "\n",
    "NNs_df   = curr_location_df_cp[['year', 'location']].copy() # data frame containing (year, location)\n",
    "dists_df = curr_location_df_cp[['year', 'location']].copy() # data frame containing distances\n",
    "\n",
    "## concatenate new data frame to above ones, to speed up\n",
    "NNs_df_new_cols, dists_df_new_cols = create_colnames(NN_count)\n",
    "\n",
    "NNs_df_helper = pd.DataFrame('-999', index=NNs_df.index,  columns=NNs_df_new_cols)\n",
    "dists_df_helper = pd.DataFrame('-999', index=dists_df.index,  columns=dists_df_new_cols)\n",
    "\n",
    "NNs_df = pd.concat([NNs_df, NNs_df_helper], axis=1)\n",
    "dists_df = pd.concat([dists_df, dists_df_helper], axis=1)\n",
    "\n",
    "del(NNs_df_helper, dists_df_helper, NNs_df_new_cols, dists_df_new_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICV = complete_hist_df.copy()\n",
    "ICV = ICV.loc[ICV['location'] == curr_location_df_cp.location.unique()[0]] # filter corresponding location\n",
    "#############################################################################\n",
    "#\n",
    "#          Normalize before doing anything\n",
    "#\n",
    "#############################################################################\n",
    "ICV_means = ICV.loc[:, numeric_feat].mean()\n",
    "ICV_stds = ICV.loc[:, numeric_feat].std()\n",
    "ICV_stds[ICV_stds.le(10**(-10))] = 1\n",
    "\n",
    "ICV = (ICV.loc[:, numeric_feat] - ICV_means) / ICV_stds\n",
    "curr_location_df_cp.loc[:, numeric_feat] = (curr_location_df_cp.loc[:, numeric_feat] - ICV_means) / ICV_stds\n",
    "complete_hist_df.loc[:, numeric_feat] = (complete_hist_df.loc[:, numeric_feat] - ICV_means) / ICV_stds\n",
    "#\n",
    "# pick numerical part of the data frame to do the operations:\n",
    "#\n",
    "complete_hist_df_numeric = complete_hist_df.loc[:, numeric_feat].copy()\n",
    "future_numeric = curr_location_df_cp.loc[:, numeric_feat].copy()\n",
    "ICV = ICV.loc[:, numeric_feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_effective_compon(ICV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 8)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ICV.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Apply PCA here and use those to find analogs\n",
    "pca = PCA(n_components = 8)\n",
    "pca.fit(ICV);\n",
    "#\n",
    "# transform data into PCA space to compute analogs\n",
    "ICV_pca = pca.transform(ICV)\n",
    "hist_pca = pca.transform(complete_hist_df_numeric)\n",
    "future_pca = pca.transform(future_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.67501134,  0.58296693,  0.82467531,  0.74516984,  1.26769116,\n",
       "        0.85198479, -0.67452197, -0.7902211 , -0.39207248, -0.27255373,\n",
       "       -0.27965518, -0.59185053,  0.27674365, -2.1516196 ,  0.81403737,\n",
       "       -1.29193397,  0.2272856 ,  0.08476283, -0.03999558,  1.23218582,\n",
       "       -0.07925489, -0.23312085,  0.12281806, -0.41657607, -0.07447846,\n",
       "       -0.72805407,  0.46049125,  0.51987213, -2.01270611,  0.6814595 ,\n",
       "        0.14134909,  0.22474309,  0.75333838,  0.15086148, -0.146713  ,\n",
       "        0.6102927 , -1.07241274])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(ICV_pca, ICV_pca.transpose())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.67501134,  0.58296693,  0.82467531,  0.74516984,  1.26769116,\n",
       "        0.85198479, -0.67452197, -0.7902211 , -0.39207248, -0.27255373,\n",
       "       -0.27965518, -0.59185053,  0.27674365, -2.1516196 ,  0.81403737,\n",
       "       -1.29193397,  0.2272856 ,  0.08476283, -0.03999558,  1.23218582,\n",
       "       -0.07925489, -0.23312085,  0.12281806, -0.41657607, -0.07447846,\n",
       "       -0.72805407,  0.46049125,  0.51987213, -2.01270611,  0.6814595 ,\n",
       "        0.14134909,  0.22474309,  0.75333838,  0.15086148, -0.146713  ,\n",
       "        0.6102927 , -1.07241274])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(ICV, ICV.transpose())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.67501134,  0.58296693,  0.82467531, ..., -0.146713  ,\n",
       "         0.6102927 , -1.07241274],\n",
       "       [ 0.58296693,  3.37156484,  3.8890029 , ..., -2.39836335,\n",
       "         0.19731406, -4.77002201],\n",
       "       [ 0.82467531,  3.8890029 ,  5.00621334, ..., -2.91636435,\n",
       "         0.74780131, -5.20816261],\n",
       "       ...,\n",
       "       [-0.146713  , -2.39836335, -2.91636435, ...,  2.74199704,\n",
       "        -0.71934626,  1.16097701],\n",
       "       [ 0.6102927 ,  0.19731406,  0.74780131, ..., -0.71934626,\n",
       "         1.62246561,  1.95746039],\n",
       "       [-1.07241274, -4.77002201, -5.20816261, ...,  1.16097701,\n",
       "         1.95746039, 13.69479919]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(ICV_pca, ICV_pca.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.random.rand(20, 3)\n",
    "np.cov(A).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.2099751   0.1602145  -0.00314589 -0.04802889  0.06953303  0.12085592\n",
      " -0.18602613  0.01842461 -0.09123304  0.00043655 -0.13665469  0.0618712\n",
      " -0.04276755  0.01870939 -0.00386734 -0.15509927 -0.01483284  0.02354324\n",
      " -0.05794508  0.05603718]\n"
     ]
    }
   ],
   "source": [
    "A = np.random.rand(20, 3)\n",
    "pca = PCA(n_components = 3)\n",
    "pca.fit(A);\n",
    "A_pca = pca.transform(A)\n",
    "A_pca_1 = sklearn.covariance.empirical_covariance(A, assume_centered=False)\n",
    "print(np.cov(A_pca)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_ICV = hist_orig[hist_orig.location == hist_orig.location[0]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_ICV_numeric = one_ICV.drop(['year', 'location', 'ClimateScenario', 'mean_escaped_Gen4'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>medianDoY</th>\n",
       "      <th>NumLarvaGens_Aug</th>\n",
       "      <th>mean_escaped_Gen1</th>\n",
       "      <th>mean_escaped_Gen2</th>\n",
       "      <th>mean_escaped_Gen3</th>\n",
       "      <th>mean_precip</th>\n",
       "      <th>mean_gdd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68</td>\n",
       "      <td>2.939262</td>\n",
       "      <td>1.319697</td>\n",
       "      <td>7.080458</td>\n",
       "      <td>1.348167</td>\n",
       "      <td>300.525</td>\n",
       "      <td>4821.668922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293</th>\n",
       "      <td>57</td>\n",
       "      <td>3.061175</td>\n",
       "      <td>1.004624</td>\n",
       "      <td>7.226781</td>\n",
       "      <td>2.052017</td>\n",
       "      <td>276.025</td>\n",
       "      <td>5031.662206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      medianDoY  NumLarvaGens_Aug  mean_escaped_Gen1  mean_escaped_Gen2  \\\n",
       "0            68          2.939262           1.319697           7.080458   \n",
       "1293         57          3.061175           1.004624           7.226781   \n",
       "\n",
       "      mean_escaped_Gen3  mean_precip     mean_gdd  \n",
       "0              1.348167      300.525  4821.668922  \n",
       "1293           2.052017      276.025  5031.662206  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_ICV_numeric_centered = one_ICV_numeric - one_ICV_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = detect_effective_compon(ICV))\n",
    "pca.fit(ICV);\n",
    "#\n",
    "# transform data into PCA space to compute analogs\n",
    "ICV_pca = pca.transform(ICV)\n",
    "hist_pca = pca.transform(complete_hist_df_numeric)\n",
    "future_pca = pca.transform(future_numeric)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
